\documentclass{article}
\title{Pure Exploration of Combinatorial Bandits}
\author{Shouyuan Chen}
\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Change "article" to "report" to get rid of page number on title page
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{Tabbing}
\usepackage{fancyhdr}
\usepackage{lastpage} 
\usepackage{extramarks}
\usepackage{chngpage}
\usepackage{soul,color}
\usepackage{graphicx,float,wrapfig}
\usepackage{afterpage}
\usepackage{abstract}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{xspace}

% In case you need to adjust margins:
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\setlength\parindent{0pt}


% Setup the header and footer
\pagestyle{fancy}
%\lhead{\LatexerName}
%\chead{\LectureClassName: \LectureTitle}
%\rhead{\LectureDate}
%\lfoot{\lastxmark}
%\cfoot{}
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\allowdisplaybreaks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some tools
\newcommand{\junk}[1]{}

\newtheorem{define}{Definition}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}


\newcommand{\Problem}{\texttt{ExpCMAB}\xspace}
\newcommand{\Rew}{\varphi}
\newcommand{\E}{\mathbb E}

\newcommand{\M}{\mathcal M}
\newcommand{\mmatch}{\mathcal M_{\mathsf{MATCH}}}
\newcommand{\mtop}{\mathcal M_{\mathsf{TOP}m}}
\newcommand{\mbandit}{\mathcal M_{\mathsf{BANDIT}m}}

\newcommand{\diff}{\mathsf{diff}}
\newcommand{\diffvalid}{\prec}
\newcommand{\B}{\mathcal B}
\newcommand{\C}{\mathcal C}
\newcommand{\del}{\backslash}

\newcommand{\RR}{\mathbb R}

%\newcommand{\vec}[1]{\mathbf #1}

\newcommand{\Bopt}{\mathcal B_{\mathsf{opt}}}
\newcommand{\Bmatch}{\mathcal B_{\mathsf{MATCH}}}
\newcommand{\Btop}{\mathcal B_{\mathsf{TOP}m}}
\newcommand{\Bbandit}{\mathcal B_{\mathsf{BANDIT}m}}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\decomp}{decomp}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Oracle}{Oracle}

\newcommand{\out}{\mathsf{Out}}


\let\Pr\undefined
\DeclareMathOperator{\Pr}{Pr}

\newcommand{\MultiIdent}{\textbf{Multi}\xspace}
\newcommand{\Matroid}{\textbf{Matroid}\xspace}
\newcommand{\Match}{\textbf{Match}\xspace}
\newcommand{\Path}{\textbf{Path}\xspace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{spacing}{1.1}
\newpage

\maketitle

\section{Preliminaries}

\subsection{Problems}

Let $n$ be the number of base arms. 
Let $\M \subseteq 2^{[n]}$ be the set of super arms. 


In this note, we consider the following cases of $\M$.

\begin{example}[Explore-$m$]
$\mtop(n)=\{M \subseteq [n] \;|\; |M|=m\}$.
This corresponds to finding the top $m$ arms from $[n]$.
\end{example}

\begin{example}[Explore-$m$-bandits]
Suppose $n=mk$. Then $\mbandit(n)$ contains all subsets $M \subseteq [n]$ with size $m$, such that 
$$ 
M\cap \{ik+1,\ldots, (i+1)k\} = 1, \quad \text{for all } i \in \{0,\ldots, m-1\}.
$$ 
This corresponds to finding the top arms from $m$ bandits, where each bandit has $k$ arms.
\end{example}

\begin{example}[Perfect Matching]
Let $G=(V,E)$ be a bipartite graph and $|E|=n$. 
For simplicity, let each edge $e\in E$ corresponds to a unique integer $i\in [n]$, and vice versa. 
Then $\mmatch(n,G)$ contains all subsets $M \subseteq [n]$ such that $M$ corresponds to a perfect matching in $G$.
\end{example}


\subsection{Diff-Sets}

\begin{define}[Diff-set]
An $n$-diff-set (or diff-set in short) is a pair of sets $c=(c_+,c_-)$, where $c_+\subseteq[n]$, $c_-\subseteq [n]$ and $c_+\cap c_-=\emptyset$.
\end{define}


\begin{define}[Difference of sets]
Given any $M_1\subseteq[n],M_2\subseteq[n]$. We define $M_1\ominus M_2 \triangleq C$, where $C=(C_+,C_-)$ is a diff-set and
$C_+ = M_1 \del M_2$ and $C_- = M_2\del M_1$.
\end{define}

\begin{define}
Denote $\diff[n]$ be the set of all possible $n$-diff-sets.
\end{define}

\begin{define}[Set operations of diff-sets] 
Let $C=(C_+,C_-), D=(D_+,D_-)$ be two diff-sets. 
We define 
$C\cap D \triangleq (C_+\cap D_+, C_-\cap D_-)$
and $C\del D \triangleq (C_+\del D_+, C_-\del D_-)$.

Further, for all $e\in [n]$, $e \in C \Leftrightarrow (e\in C_+)\vee(e\in C_-)$.
And $|C|\triangleq |C_+|+|C_-|$.
\end{define}

\begin{define}[Valid diff-set]
Given a set $M \subseteq [n]$ and a diff-set $C=(C_+,C_-)$, we call $C$ a \emph{valid diff-set} for $M$, iff $C_+ \cap M = \emptyset$ and $C_- \subseteq M$.
In this case, we denote $C\diffvalid M$.
\end{define}

\begin{define}[Negative diff-set]
Given a diff-set $A=(A_+,A_-)$, we define $\neg A=(A_-,A_+)$.
\end{define}


\subsubsection{diff-set operations}

\begin{define}[Operators $\oplus$ and $\ominus$]
Given any $M \subseteq [n]$ and $C \in \diff[n]$.
If $C\diffvalid M$, we define operator $\oplus$ such that $M \oplus C \triangleq M\del C_- \cup C_+ $.
On the other hand if $\neg C\diffvalid M$, we define operator $\ominus$ such that $M \ominus C \triangleq M\oplus (\neg C) = 
M\del C_+ \cup C_- $.
\end{define}


\begin{define}
Given two diff-sets $A=(A_+,A_-)$ and $B=(B_+,B_-)$.
We denote $B \diffvalid A$, if and only if $B_+\cap A_+ = \emptyset$ and $A_+\cap A_-=\emptyset$.
\end{define}

\begin{define}
Given two diff-sets $A=(A_+,A_-)$ and $B=(B_+,B_-)$. 
If $B\diffvalid A$, we define $A\oplus B = ( (A_+\cup B_+)\del(A_-\cup B_-), (A_-\cup B_-)\del(A_+\cup B_+))$.
\end{define}

\begin{lemma}
Given two diff-sets $A=(A_+,A_-)$ and $B=(B_+,B_-)$. 
If $B\diffvalid A$, then 
$A\oplus B$ is a diff-set.
\end{lemma}

\begin{proof}
Let $C=A\oplus B$.
By definition, we have $C_+ = (A_+\cup B_+)\del(A_-\cup B_-)$ and $C_-=(A_-\cup B_-)\del(A_+\cup B_+)$.

We only need to show that $C_+\cap C_-=\emptyset$.
\begin{align*}
	C_+ \cap C_- &= \big((A_+\cup B_+)\del(A_-\cup B_-)\big)\cap\big((A_-\cup B_-)\del(A_+\cup B_+)\big)\\
			     &= (A_+\cup B_+)\cap\big((A_-\cup B_-)\del(A_+\cup B_+) \del (A_-\cup B_-)\big) \\
			     &= \emptyset.
\end{align*}
\end{proof}

\begin{lemma}
\label{lemma:diff-set-algebra}
Given two diff-sets $A=(A_+,A_-)$ and $B=(B_+,B_-)$. 
If there exists $M\subseteq [n]$ such that $A \diffvalid M$, and $B \diffvalid (M\oplus A)$,
then $B \diffvalid A$ and $(M\oplus A \oplus B)\ominus M = A\oplus B$.
\end{lemma}

\begin{proof}
We first show that $B\diffvalid A$.
Since $B \diffvalid (M\oplus A)$, we know that 
$B_+\cap (M\del A_- \cup A_+) = \emptyset$.
Therefore, we have
\begin{align*}
	\emptyset &= B_+\cap (M\del A_-\cup A_+) \\
			  &= (B_+\cap (M\del A_-))\cup (B_+\cap A_+)
\end{align*}
We see that $B_+\cap A_+=\emptyset$.

On the other hand, we have $B_-\subseteq (M\del A_-\cup A_+)$, therefore 
\begin{align*}
	B_-\cap A_- &\subseteq (M\del A_-\cup A_+)\cap A_- \\
				&= (M\del A_- \cap A_-)\cup (A_+\cap A_-) \\
				&= \emptyset.
\end{align*}
Hence we proved that $B\diffvalid A$.

Define $D=(M\oplus A \oplus B)\ominus M$ and write $D=(D_+,D_-)$. Then,
\begin{align*}
D_+ &= (M\oplus A\oplus B) \del M\\ 
    &= (M\del A_-\cup A_+\del B_-\cup B_+) \del M\\
    &= (A_+\cup B_+)\del(A_-\cup B_-).
\end{align*}
Similarly, we have
\begin{align*}
D_- &= M\del (M\oplus A\oplus B)\\ 
    &= M\del (M\del A_-\cup A_+\del B_-\cup B_+)\\
    &= (A_-\cup B_-)\del (A_+\cup B_+).
\end{align*}
\end{proof}

\subsubsection{Diff-set class}

\begin{define}[Decomposition of diff-set]
Given $\B\subseteq \diff[n]$ and $D\in \diff[n]$, 
a decomposition of $D$ on $\B$ is a set $\{b_1,\ldots,b_k\} \subseteq \B$ satisfying the following
\begin{enumerate}
  \item For all $i\in[k]$ and $j\in [k]$, we write $b_i=(b_i^+,b_i^-)$ and $b_j=(b_j^+,b_j^-)$. Then, the following holds
  $b_i^+\cap b_j^+=\emptyset$, $b_i^+\cap b_j^-=\emptyset$, $b_i^-\cap b_j^+ =\emptyset$ and $b_i^-\cap b_j^-=\emptyset$.
  \item $D=b_1 \oplus b_2 \oplus \ldots b_k$.
\end{enumerate}
\end{define}

\begin{lemma}
Given $\B\subseteq \diff[n]$ and $D\in \diff[n]$.
Let $\{b_1,\ldots,b_k\} \subseteq \B$ be a decomposition of $D$ on $\B$.
Then,
\begin{enumerate}
\item Let $D=(D_+,D_-)$ and for all $i \in [k]$, we write $b_i = (b_i^+,b_i^-)$.
	  Then $D_+=b_1^+ \cup \ldots \cup b_k^+$ and $D_-=b_1^-\cup\ldots\cup b_k^-$.
\item For all $M\subseteq [n]$, if $D\diffvalid M$, then, for all $i\in [k]$, we have $b_i \diffvalid M$.
\end{enumerate}
\end{lemma}

\begin{proof}
We prove (1) by induction.
Let $D_i = b_1 \oplus \ldots \oplus b_i$ and write $D_i=(D_i^+, D_i^-)$.
We show that $D_i^+=\bigcup_{j=1}^i b_i^+$ and $D_{i-}=\bigcup_{j=1}^i b_i^-$ for all $i\in[k]$.
For $i=1$, this is trivially true.
Then, assume that this is true for some $i>1$.
By definition $D_{i+1}=D_{i}\oplus b_{i+1}$, hence $D_{i+1}^+=(D_i^+ \cup b_{i+1}^+)\del(D_i^- \cup b_{i+1}^-)$.
Note that 
\begin{align*}
(D_i^-\cup b_{i+1}^-)\cap(D_i^+ \cup b_{i+1}^+) &= (D_i^-\cap D_i^+)\cup(D_i^- \cap b_{i+1}^+)\cup(b_{i+1}^- \cap D_i^+)\cup(b_{i+1}^- \cap b_{i+1}^+)\\
		&= (D_i^- \cap b_{i+1}^+) \cup (b_{i+1}^- \cap D_i^+) \\
		&= \left(\left(\bigcup_{j=1}^i b_{j}^-\right) \cap b_{i+1}^+ \right) \cup \left(\left(\bigcup_{j=1}^i b_{j}^+\right) \cap b_{i+1}^- \right)\\
		&= \emptyset.
\end{align*}
Hence $D_{i+1}^+=D_i^+ \cup b_{i+1}^+$. We can use the same method to show that $D_{i+1}^-=D_i^- \cup b_{i+1}^-$.

Next, we prove (2) using (1).
To show that $b_i\diffvalid M$, we only need to show that $b_i^+ \cap M = \emptyset$ and $b_i^- \subseteq M$.
Since $D\diffvalid M$, we know that $D_+\cap M=\emptyset$ and $D_-\subseteq M$.
By (1), we see that $b_i^+\subseteq D_+$ and $b_i^-\subseteq D_-$.
Therefore, we have $(b_i^+\cap M) \subseteq (D_+\cap M) = \emptyset$ and $b_i^-\subseteq D_- \subseteq M$.

\end{proof}


\begin{define}[diff-set class]
\label{define:diff-class}
Given $\M \subseteq 2^{[n]}$. $\B \subseteq \diff[n]$ is a diff-set class for $\M$, if the following hold.
\begin{enumerate} 
\item $(\emptyset,\emptyset)\not\in \B$.
\item For all $M\in \M$ and for all $b\in \B$, if $b\diffvalid M$, then $M\oplus b \in \M$.
\item For all $M_1 \in \M$ and $M_2\in \M$, where $M_1\not=M_2$. 	
	  Let $D=M_1\ominus M_2$.  
	  Then, there exists a decomposition of $D$ on $\B$.
\end{enumerate}
\end{define}

\begin{define}[Rank of diff-set class]
Let $\B \subseteq [n]$ be a diff-set class for some $\M$. We define
$$
\rank(\B) \triangleq \max_{b \in \B} |b|.
$$
\end{define}

\begin{example}[diff-set class for Explore-$m$]
One diff-set class $\B$ for $\mtop(n)$ is given by
$$
\B=\{(\{b_1\},\{b_2\}) \;|\; b_1\not=b_2, b_1\in[n], b_2\in[n]\}.
$$
Proof omitted.
Further, we see that $\rank(\B)=2$.
\end{example}

\begin{example}[diff-set class for Explore-$m$-badit]
Let $n=mk$.
One diff-set class $\B$ for $\mbandit(n)$ is given by
$$
\B=\{(\{b_1\},\{b_2\}) \;|\; b_1\not=b_2, \exists i\in\{0,\ldots,k-1\}, b_1\in \{ik+1,\ldots, (i+1)k\}, b_2\in\{ik+1,\ldots, (i+1)k\}\}.
$$
Proof omitted.
Further, we see that $\rank(\B)=2$.
\end{example}


\begin{example}[diff-set class for Perfect Matching]
One diff-set class $\B$ for $\mmatch(n, G)$ is the set of all augmenting cycles of $G$. 
More specifically,
$$
\B=\{(b_+,b_-) | b_+\cup b_- \text{ is a cycle of } G\}.
$$

Note $\rank(\B)\le n$.
\end{example}

\subsection{Weights and confidence bounds}

\begin{define}[Weight functions]
Define function $w: [n] \rightarrow \RR^+$ which represents the weight of each base arm. 
Further, we slight abuse the notations, and extend the definition of $w$ to diff-sets and sets as follows.
\begin{enumerate}
\item For all $M \subseteq [n]$, we denote $w(M) = \sum_{e\in M} w(e)$.
\item For all $b=(b_+,b_-) \in \diff[n]$, we denote $w(b) = \sum_{e\in b_+} w(e)-\sum_{e\in b_-}w(e)$.
\end{enumerate}
\end{define}


\begin{lemma}
\label{lemma:weight-diff-simple}
Let $c\in \diff[n],d\in \diff[n]$. Let $w$ be a weight function.
Then,
\begin{equation}
w(c\cup d) = w(c)+w(d)-w(c\cap d).
\end{equation}
\end{lemma}

\begin{proof}
Let $c=(c_+,c_-)$ and $d=(d_+,d_-)$.
We have
\begin{align}
w(c\cup d) &= w(c_+\cup d_+)-w(c_-\cup d_-)\\
           &= w(c_+)+w(d_+)-w(c_+\cap d_+)-w(c_-)-w(d_-)+w(c_- \cap d_-)\\
           &= w(c)+w(d)-(w(c_+\cap d_+)-w(c_-\cap d_-))\\
           &= w(c)+w(d)-w(c\cap d).
\end{align}
\end{proof}

\begin{define}[Mean weight $\bar w_t$, sample size $n_t$]
Given $t>0$. 
Define $\bar w_t$ be a weight function such that, for all $e\in[n]$, $\bar w_t(e)$ equals to the empirical mean of $e$ up to round $t$.
Let $n_t: [n] \rightarrow \mathbb N$, such that $n_t(e)$ equals to number of plays of base arm $e$ up to round $t$.
\end{define}

\begin{define}[Confidence radius $\rad_t$]
Given $n$ and $t>0$.
Define $\rad_t:[n]\rightarrow \RR^+$ satisfying, for all $e\in[n]$,
\begin{equation}
\label{eq:define-confidence-radius}
\rad_t(e) = c_{\rad}\log\left(\frac{c_\delta nt^2}\delta\right)\frac{1}{\sqrt{n_t(e)}},
\end{equation}
where $c_{\rad} > 0$  and $c_\delta>0$ are some universal constants (specify later) and $\delta > 0$ is a parameter.

We extend the notation of $\rad_t$ to diff-sets and sets as follows.
\begin{enumerate}
\item For all $M \subseteq [n]$, $\rad_t(M) \triangleq \sum_{e\in M} \rad_t(e)$.
\item For all $b=(b_+,b_-)\in \diff[n]$, $\rad_t(b) \triangleq \rad_t(b_+)+\rad_t(b_-)$.
\end{enumerate}

\end{define}

\begin{define}[UCB $w_t^+$]
Define $w^+_t: [n] \rightarrow \RR^+$, s.t., for all $e\in[n]$,  
$$ w^+_t(e)=\bar w_t(e)+\rad_t(e).$$

We extend the notation of $w_t^+$ to diff-sets and sets as follows.
\begin{enumerate}
\item For all $M \subseteq [n]$, $w_t^+(M) \triangleq \bar w_t(M)+\rad_t(M)$.
\item For all $b=(b_+,b_-)\in \diff[n]$, $w_t^+(b) \triangleq \bar w_t(b)+\rad_t(b)$.
\end{enumerate}

\end{define}

\begin{lemma}
\label{lemma:conf}
Define random event 
$$
\xi = \left\{\forall e\in[n]\; \forall t>0, |\bar w_t(e)-w(e)|\le \rad_t(e) \right\}.
$$
Then, there exist constants $c_{\rad}$ and $c_\delta$,
$$
\Pr[\xi] \ge 1-\delta.
$$
\end{lemma}
\begin{proof}
Hoeffding inequality and union bound.
\end{proof}



\begin{corollary}
\label{corr:conf}
$$
\xi \implies \forall t,\forall e\in[n] \; w_t^+(e) \ge w(e).
$$
$$
\xi \implies \forall t,\forall M\subseteq[n],\; w_t^+(M) \ge w(M).
$$
$$
\xi \implies \forall t,\forall b\in\diff[n]\; w_t^+(b) \ge w(b).
$$
\end{corollary}

\junk{
\begin{lemma}
Let $c\in \diff[n],d\in \diff[n]$. Let $\rad_t$ be a radius function.
Then,
\begin{equation}
\rad_t(c\cup d) = \rad_t(c)+\rad_t(d)-\rad_t(c\cap d).
\end{equation}
\end{lemma}

\begin{proof}
Let $c=(c_+,c_-)$ and $d=(d_+,d_-)$.
We have
\begin{align}
\rad_t(c\cup d) &= \rad_t(c_+\cup d_+) + \rad_t(c_-\cup d_-)\\
			  &= \rad_t(c_+)+\rad_t(d_+)-\rad_t(c_+\cap d_+)
			    +\rad_t(c_-)+\rad_t(d_-)-\rad_t(c_-\cap d_-)\\
			  &= \rad_t(c)+\rad_t(d)-\rad_t(c\cap d).
\end{align}
\end{proof}
}

\subsection{Properties of $\rad_t$}

\begin{lemma}
\label{lemma:rad-diff-simple}
Let $c\in \diff[n],d\in \diff[n]$.
Then
\begin{equation}
\rad_t(c\del d) = \rad_t(c)-\rad_t(c\cap d).
\end{equation}
\end{lemma}

\begin{proof}
Let $c=(c_+,c_-)$ and $d=(d_+,d_-)$.
We have
\begin{align*}
\rad_t(c\del d) &= \rad_t(c_+\del d_+)+\rad_t(c_-\del d_-)\\
			    &= \rad_t(c_+)-\rad_t(c_+\cap d_+)+\rad_t(c_-)-\rad_t(c_-\cap d_-)\\
			    &= \rad_t(c)-\rad_t(c\cap d).
\end{align*}

\end{proof}


\begin{lemma}
\label{lemma:diff-algebra-rad}

Let $C=(C_+,C_-)$ and $D=(D_+,D_-)$ be two diff-sets.
If $D \diffvalid C$, then
$$
\rad_t(C \oplus D) = \rad_t(C)+\rad_t(D)-2\rad_t(C_+ \cap D_-)-2\rad_t(C_- \cap D_+).
$$

In addition, if $\neg D \diffvalid C$, then
$$
\rad_t(C \ominus D) = \rad_t(C)+\rad_t(D)-2\rad_t(C_+ \cap D_+)-2\rad_t(C_- \cap D_-).
$$
\end{lemma}

\begin{proof}
We prove the first part of the lemma. The second part follows from the first part and the definition of $\neg D$.

By definition, we have $C\oplus D = ( (C_+ \cup D_+) \del (C_-\cup D_-), (C_-\cup D_-) \del (C_+\cup D_+) )$.
Hence, we have 
\begin{align}
\rad_t((C_+ \cup D_+) \del (C_-\cup D_-)) &= \rad_t(C_+ \cup D_+)-\rad_t( (C_+\cup D_+)\cap(C_-\cup D_-)) \\
							              &= \rad_t(C_+)+\rad_t(D_+)-\rad_t( (C_+\cup D_+)\cap(C_-\cup D_-)),						
\end{align}
where the second equality holds due to $C_+\cap D_+=\emptyset$ by the definition of $D\diffvalid C$.

Similarly, we have
$$
\rad_t((C_- \cup D_-) \del (C_+\cup D_+)) = \rad_t(C_-)+\rad_t(D_-)-\rad_t( (C_+\cup D_+) \cap(C_-\cup D_-)).
$$

Combine both equalities, we have
\begin{align}
  \rad_t(C \oplus D) &= \rad_t((C_+ \cup D_+) \del (C_-\cup D_))+\rad_t((C_- \cup D_-) \del (C_-\cap D_+))\\
  				     &=	\rad_t(C_+)+\rad_t(D_+)+\rad_t(C_-)+\rad_t(D_-)-2\rad_t( (C_+\cup D_+) \cap(C_-\cup D_-))\\
  				     &= \rad_t(C)+\rad_t(D)-2\rad_t( (C_+\cup D_+) \cap(C_-\cup D_-)).
\end{align}


\end{proof}


\section{Pure Exploration of Combinatorial Bandits}


\textbf{\Problem: problem formulation.}
Suppose that the arms are numbered $1,2,\ldots,n$.
Each arm $e\in[n]$ is associated with a reward distribution $\Rew_e$. 
We assume that all reward distributions are $b$-subgaussian, i.e. ().
Notice that all distributions that are supported on $[0,b]$ are $b$-subgaussian distributions [].
Let $w(e)$ denote the expected reward of arm $e$, i.e. $w(e)=\E_{X\sim \Rew_e}[X]$.
In addition, for any set of arms $M\subseteq [n]$, we define $w(M) = \sum_{e\in M} w(e)$ as the sum of expected rewards of arms that belong to $M$.

The learning problem of pure exploration combinatorial bandit can be formalized as a game between a learner and a stochastic environment.
At the beginning of the game, the learner is given a collection of feasible sets $\M\subseteq 2^{[n]}$ which corresponds to some combinatorial problem. 
And the reward distributions $\{\Rew_e\}_{e\in[n]}$ are unknown to the learner.
Then, the game is played for multiple rounds;
on each round $t$, the learner pulls an arm $p_t\in [n]$ and observes a reward sampled from the associated reward distribution $\Rew_{p_t}$.
The game continues until certain stopping condition is satisfied (specify later).
After the game finishes, the learner is asked to output a set of arms $\out \in \M$ which approximately maximizes the sum of expected weight, i.e. $w(M_*) - w(\out) \le \epsilon$, where we denote $M_*=\argmax_{M\in \M} w(M)$ to be the optimal set of arms.
For the sake of simplicity, we shall assume that the optimal set $M_*$ is unique throughout the paper.
Notice that, if $\epsilon = 0$, then the learner is required to identify the optimal set, i.e. $\out = M_*$.



\textbf{Fixed confidence and fixed budget.} We consider two different settings: \emph{fixed confidence} and \emph{fixed budget}. 
In the fixed confidence setting, the learner aims to achieve a fixed confidence about the optimality of the returned set using a small number of samples (pulls).
Specifically, given a confidence parameter $\delta$, the learner need to guarantee that $\Pr[w(M_*)-w(\out) \le \epsilon] \ge 1-\delta$ and the performance is evaluated by the number of pulls used by the learner.
Notice that the learner can stop the game at any point in this setting.
In the fixed budget setting, the learner tries to minimize the probability of error $\Pr[w(M_*)-w(\out) > \epsilon]$ by using a fixed number of samples, i.e. the game stops after a fixed number of rounds.
In this case, the learner's performance is measured by the probability of error.


\textbf{Applications.} 
Our formulation of the \Problem problem covers many online learning tasks.
We consider the following applications as running examples.

\begin{itemize}
\item \MultiIdent.
\item \Match.
\item \Path.
\end{itemize}

\textbf{Useful notations.}


\section{Algorithm and Main Results}
Our main contribution is an algorithm for solving the \Problem problem.
Our algorithm 

%In this section, we describe our algorithm for pure exploration combinatorial bandit problem.
Then, we analyze the sample complexity and the probability of error of our algorithm.


%Many common combinatorial problems admit computationally efficient oracles.

\textbf{Maximization oracle.}
For most non-trivial combinatorial problems, the size of the collection of feasible sets $\M$ is exponential in $n$.
%Hence, the definition of $\M$ 
Therefore, the learning algorithm needs a succinct representation of $\M$.
In particular, we allow the learning algorithm to use a \emph{maximization oracle} which can find the optimal set $M\in \M$ when the expected reward of each arm is known.
Specifically, we assume that there exists an oracle which takes expected rewards $\{w(1),\ldots,w(n)\}$ as input and returns a set $\Oracle(w) = \argmax_{M\in \M} w(M)$.
It is clear that a large class of combinatorial problems admit efficient maximization oracles.


\textbf{Algorithm.} 
Our algorithm works for both fixed confidence and fixed budget settings.
In either settings, the behaviors of our algorithm only differ in the construction of confidence radius and the stopping condition.
In the following, we describe the procedure of our algorithm.
Our algorithm maintains the empirical mean $\bar w_t(e)$ and a confidence radius $\rad_t(e)$ for each arm $e\in[n]$ and each round $t$.
The construction of confidence radius ensures that $|w(e)-\bar w_t(e)| \le \rad_t(e)$ holds with high probability for each arm $e \in [n]$ and each round $t>0$.
At each round $t$, our algorithm accesses the maximization oracle twice. 
The first access to the oracle computes the set $M_t=\argmax_{M\in \M} \bar w_t(M)$.
Notice that $M_t$ is the ``best'' set according to the empirical means $\bar w_t$.
% which maximizes empirical means $\bar w_t$ up to  $t$.
Then, in order to explore possible refinements of $M_t$, the algorithm uses the confidence radius to compute an adjusted expectation vector $\tilde w_t$ in the following way: for each arm $e \in M_t$, $\tilde w_t(e)$ equals to the lower confidence bound $\tilde w_t(e) = \bar w_t(e)-\rad_t(e)$; and for each arm $e\not\in M_t$, $\tilde w_t(e)$ equals to the upper confidence bound $\tilde w_t(e)=\bar w_t(e)+\rad_t(e)$.
Intuitively, the adjusted expectation vector $\tilde w_t$ penalizes arms belonging to $M_t$ and encourages exploring arms out of $M_t$.
The algorithm then calls the oracle using the adjusted expectation vector $\tilde w_t$ as input, which returns another set $\tilde M_t = \argmax_{M\in \M} \tilde w_t(M)$.
The algorithm stops if $\tilde w_t(\tilde M_t)-\tilde w_t(M_t) \le \epsilon$ or the budget of samples is exhausted, i.e. $t=T$, in the fixed budget setting.
In either cases, the algorithm outputs $\out=M_t$ as result.
Otherwise, the algorithm plays the arm belonging to the symmetric difference $(\tilde M_t \del M_t) \cup (M_t \del \tilde M_t)$ with the largest confidence radius in the end of round $t$.
The pseudo-code of the algorithm is shown in Algorithm~\ref{algo:pac}. 

\begin{algorithm}[htbp]
\begin{algorithmic}[1]
\Require Confidence parameter: $\delta \in (0,1)$; Tolerance parameter: $\epsilon >0$; Maximization oracle: $\Oracle(w)$.
\Statex \textbf{Initialize:} Play each arm $e \in [n]$ once. Initialize empirical means $\bar w_{n}(e)$ and set $T_{n}(e) \gets 1$ for all $e$.
\For{$t=n,n+1,\ldots$}
	\State $M_t \gets \Oracle(\bar w_t)$
	\For{$e\in [n]$}
		\If {$e\in M_t$}
			\State $\tilde w_t(e) \gets \bar w_t(e)-\rad_t(e)$
		\Else
			\State $\tilde w_t(e) \gets \bar w_t(e)+\rad_t(e)$
		\EndIf
	\EndFor
	\State $\tilde M_t \gets \Oracle(\tilde w_t)$
	\If{$\tilde w_t(\tilde M_t)-\tilde w_t(M_t) \le \epsilon$}
		\State $\out \gets M_t$
		\State \textbf{return} $\out$
	\EndIf
	\State $p_t \gets \argmax_{(\tilde M_t \del M_t) \cup (M_t \del \tilde M_t)} \rad_t(e)$\label{algo:step:D}
	\State Play $p_t$ and observe the reward
	\State Update empirical mean $\bar w_{t+1}(p_t)$
	\State Update $T_{t+1}(p_t)\gets T_{t}(p_t)+1$ and $T_{t+1}(e) \gets T_{t}(e)$ for every $e\not=p_t$
	\EndFor
\end{algorithmic}
\caption{ExploreComb: Pure exploration algorithm for combinatorial bandits}
\label{algo:pac}
\end{algorithm}


\subsection{Analysis}
In this part, we analyze the performance of Algorithm~\ref{algo:pac} for both fixed confidence and fixed budget settings. 


\textbf{Gap.} We begin with defining a natural complexity measure of the \Problem problem. 
For each arm $e \in [n]$, we define gap $\Delta_e$ as
\begin{equation}
\label{eq:define-delta}
\Delta_e = \begin{cases}
			   w(M_*)-\max_{M\in \M: e\in M} w(M) & \text{if } e\not \in M_*, \\
			   w(M_*)-\max_{M\in \M: e\not \in M} w(M) & \text{if } e\in M_*.
			\end{cases}
\end{equation}
By this definition of gap $\Delta_e$, for each arm $e\not\in M_*$, $\Delta_e$ represents the gap between the optimal set $M_*$ and the best set that includes arm $e$; and, for each arm $e\in M_*$, $\Delta_e$ is the sub-optimality of the best set that does not include arm $e$.
We notice that, for many combinatorial problems, the definition Eq.~\eqref{eq:define-delta} naturally reflects the hardness of an arm.
().
Figure X illustrates these interpretations.



\textbf{Exchange sets.}



\subsection{Lower bounds}

In this part, we establish a problem dependent lower bound on the sample complexity of the \Problem problem. 
To state our results, we first define the notion of \emph{$\delta$-correct algorithm} as follows.
For any $\delta \in (0,1)$, we call an algorithm $\mathbb A$ a $\delta$-correct algorithm if, for any expected reward $\vec w$, the probability of error of $\mathbb A$ is at most $\delta$, i.e $\Pr[w(M_*)-w(\out) \ge \epsilon] \le \delta$, where $\out$ is the output of algorithm $\mathbb A$.

Our next theorem shows shat, for any combinatorial problem $\M$, any expected rewards $\vec w$ and any $\delta$-correct algorithm $\mathbb A$, algorithm $\mathbb A$ must use at least $\tilde\Omega\left(\sum_{e} \frac{1}{\Delta_e^2}\right)$ samples in expectation.

\begin{theorem}
Fix any $\M\subseteq 2^{[n]}$ and any vector $\vec w \in \RR^n$.
Suppose that, for each arm $e\in [n]$, the reward distribution $\Rew_e$ is given by $\Rew_e=\mathcal N(w(e),1)$, where $\mathcal N(\mu, \sigma^2)$ denotes a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. 
Then, for any $\delta \in (0,e^{-16}/4)$ and any $\delta$-correct algorithm $\mathbb A$, we have
$$
\E[T] \ge \sum_e \frac{1}{16\Delta_e^2}\log(1/4\delta),
$$
where 
$T$ denote the number of total samples used by algorithm $\mathbb A$ and
$\Delta_e$ is defined in Eq.~\eqref{eq:define-delta}.
\label{theorem:lower-bound}
\end{theorem}

Now, we compare the sample complexity of Algorithm~\ref{algo:pac} to the lower bound provided in Theorem~\ref{theorem:lower-bound} on our running examples \MultiIdent, \Matroid, \Match and \Path.
For clarity, we consider the case that $\epsilon=0$ which corresponds to the learning problem of finding the optimal set.
We see that Algorithm~\ref{algo:pac} uses at most $\tilde O(\sum_{e} \rank(\B)^2/\Delta_e^2)$ samples.
Notice that, for \MultiIdent and \Matroid problems, Lemma~X shows that $\rank(\B)=2$.
Hence, for these two problems, we see that Algorithm~\ref{algo:pac} achieves optimal sample complexity which is tight up to logarithmic factors.

On the other hand, for $\Match(V,E)$ and $\Path(V,E)$, Lemma~X indicates that $\rank(\B)=|V| \le n$.
This means that the gap between our algorithm and this lower bound is a factor of $|V|^2$.
Notice this gap only depends on the underlying combinatorial structure of $\M$ and is independent of expected rewards $\vec w$. 
This means that  the sample complexity of Algorithm~\ref{algo:pac} has an optimal dependency on $\{\Delta_e\}_{e\in[n]}$.

However, we still remain to investigate the necessity of the dependency on $\rank(\B)$ of Algorithm~\ref{algo:pac}.
To this end, we provide evidence showing that the sample complexity of any $\delta$-correct algorithm should be related to size of exchange sets.
%Furthermore, the lower bound can be improved to be dependent on the size of exchange sets.
In fact, we show that, for any optimal exchange set $b\in\Bopt$ and any $\delta$-correct algorithm, the algorithm  must spend $\tilde \Omega\left(|b|^2/w(b)^2\right)$ samples on the arms belonging to $b$.
This result is formalized in the following theorem.
\begin{theorem}
Fix any $\M\subseteq 2^{[n]}$ and any vector $\vec w \in \RR^n$.
Suppose that, for each arm $e\in [n]$, the reward distribution $\Rew_e$ is given by $\Rew_e=\mathcal N(w(e),1)$, where $\mathcal N(\mu, \sigma^2)$ denotes a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. 
Fix any $\delta \in (0,e^{-16}/4)$
and any $\delta$-correct algorithm $\mathbb A$.

Then, for any $b \in \Bopt$, we have
$$
\E[T_b] \ge \frac{|b|^2}{32w(b)^2}\log(1/4\delta),
$$
where $T_b$ denotes the number of samples of arms belonging to $b$ used by algorithm $\mathbb A$.
\end{theorem}
Notice that 



\section{Proof of Main Results}
\begin{define}[Optimal diff-sets]
Given a diff-set class $\B$ and the optimal set $M_*$.
We define $\Bopt$ as a subset of $\B$, and for all $b\in \B$, $b\in \Bopt$ if and only if,
there exists $M\not=M_*$ and $M_* \ominus M$ can be decomposed as $b,b_1,\ldots,b_k$ on $\B$.
\end{define}

\begin{define}[Hardness $\Delta_e$ of base arm $e$]
For each $e\in [n]$, we define its hardness $\Delta_e$ as follows
$$
\Delta_e = \min_{b\in \Bopt, e\in b} \frac{1}{\rank(\B)} w(b).
$$
\end{define}

\begin{define}[Sufficient exploration]
For all $t>0$, we define $E_t^3 \subseteq [n]$, such that, for all $e\in[n]$
$e\in E_t^3$ if and only if $\rad_t(e) < \frac{1}{3} \Delta_e$.
\end{define}

\begin{corollary}
For all $t>0$ and $e\in[n]$
$$ n_t(e) \ge O(\frac{1}{\Delta_e^2}\log(\Delta_e n/\delta)) \implies e\in E_t^3.$$
\end{corollary}

\begin{theorem}
With probability at least $1-\delta$,
the algorithm returns $M_*$,
and the number of samples used by the algorithm are at most
$$
\sum_{e\in [n]} \Delta_e^{-2}\log(\Delta_e n/\delta).
$$
\end{theorem}


\begin{theorem}
Given confidence parameter $\delta \in (0,1)$, tolerance parameter $\epsilon \ge 0$, number of arms $n$ and a combinatorial problem instance $\mathcal M \subseteq 2^{[n]}$.
Let oracle $\Oracle(w)$ be a maximization oracle associated with $\mathcal M$ such that
$\Oracle(w) = \argmax_{M \in \mathcal M} w(M)$, where $w: 2^{[n]} \rightarrow R$ is a weight function.

Then, with probability at least $1-\delta$, the output $\out$ of Algorithm~\ref{algo:pac} satisfies
$
w(M_*)-w(\out) \le \epsilon,
$
where $M_* = \argmax_{M\in \M} w(M)$ is the optimal set.
In addition, the number of samples $T$ used by the algorithm satisfies
$$
T \le \mathbf H_\epsilon \log\left(\frac{n}{\delta}\mathbf H_\epsilon\right),
$$
where
$$
\mathbf H_\epsilon = \sum_{e\in[n]} \min\left\{\frac{\rank(\B)^2}{\Delta_e^2}, \frac{n^2}{\epsilon^2}\right\}.
$$
\end{theorem}

\begin{lemma}
For any arm $e \in [n]$ and any round $t > n$ after initialization, if $\rad_t(e) \le \max\left\{\frac{\Delta_e}{3\rank(\B)}, \frac{\epsilon}{n}\right\}$,
then arm $e$ will not be played on round $t$, i.e. $p_t\not= e$.
\end{lemma}
\begin{proof}
If $\rad_t(e) \le \frac{\Delta_e}{3\rank(\B)}$, then we can apply Lemma~\ref{lemma:key-technical} which immediately gives that $p_t\not= e$.
Hence, we only need to prove the case that $\rad_t(e) \le \frac{\epsilon}{n}$.
By the definition of $p_t$, we know that for each $i\in D_t$, we have $\rad_t(i) \le \rad_t(e) \le \frac{\epsilon}{n}$.
Summing up all $i\in D_t$, we obtain
\begin{equation}
\rad_t(D_t) \le \epsilon.
\label{eq:pac-key-0-1}
\end{equation}
Next, we notice that the definition of $M_t$ gives that $\bar w_t(M_t)=\max_{M\in \M} \bar w_t(M) \ge \bar w_t(M_t \oplus D_t)$.
This means that
\begin{equation}
\bar w_t(D_t) = \bar w_t(M_t\oplus D_t)-\bar w_t(M_t) \le 0.
\label{eq:pac-key-0-2}
\end{equation}
Using the above inequalities, we have.
\begin{align}
  w_t^+(D_t) &= \bar w_t(D_t)+\rad_t(D_t) \label{eq:pac-key-1}\\
  			 &\le \bar w_t(D_t)+\epsilon \label{eq:pac-key-2}\\
  			 &\le \epsilon, \label{eq:pac-key-3}
\end{align}
where Eq.~\eqref{eq:pac-key-1} follows from the definition of $w_t^+(\cdot)$; 
Eq.~\eqref{eq:pac-key-2} follows from Eq.~\eqref{eq:pac-key-0-1};
Eq.~\eqref{eq:pac-key-3} holds since Eq.~\eqref{eq:pac-key-0-2}.
\end{proof}

\begin{lemma}
\label{lemma:pac-correct}
If Algorithm~\ref{algo:pac} stops, then $w(M_*)-w(\out) \le \epsilon$.
\end{lemma}

\begin{proof}
Suppose that $\out \not= M_*$.
Suppose that the algorithm stops on round $T$, we know that $\out = M_T$.
Consider the diff-set $D=M_* \ominus M_T$ and the diff-set $D_T$ as defined in Step~\ref{algo:step:D} of Algorithm~\ref{algo:pac}.
By Lemma~Z, we see that 
\begin{equation}
w_T^+(D_T) = \max_{C: C \diffvalid M_T} w_T^+(C) \ge w_T^+(D).
\label{eq:pac-correct-0}
\end{equation}
On the other hand, the stopping condition of Algorithm~\ref{algo:pac} gives that
\begin{align}
\epsilon &\ge \tilde w_T(\tilde M_T)-\tilde w_T(M_T) \nonumber \\
		 &= w_T^+(D_T) \ge w_T^+(D) \label{eq:pac-correct-1} \\
		 &\ge w(D) = w(M_*)-w(M_T), \label{eq:pac-correct-2}
\end{align}
where Eq.~\eqref{eq:pac-correct-1} follows from Eq.~\eqref{eq:pac-correct-0}; Eq.~\eqref{eq:pac-correct-2} follows from the assumption that event $\xi$ occurred.


\end{proof}


Unless specified, we shall assume the random event $\xi$ (defined in Lemma~\ref{lemma:conf}) holds in all the following proofs.


\begin{lemma}
\label{lemma:correct}
For any $t>0$, if the algorithm terminates on round $t$, then $M_t=M_*$.
\end{lemma}

\begin{proof}
Suppose $M_t \not= M_*$. Then $w(M_*)>w(M_t)$. 
Then, there exists $b \in \B$ such that $b \diffvalid M_t$ and $w(b)>0$.
On the other hand, by Corollary~\ref{corr:conf}, we have $w_t^+(b) > w(b)$.
Hence $w_t^+(b)>0$. This contradicts to the stopping condition of our algorithm.
\end{proof}

\begin{lemma}
\label{lemma:key-technical}
For any $t>0$.
If $e\in E_t^3$, then $p_t\not= e$.
\end{lemma}

\begin{proof}
Suppose that $p_t=e$. 
Let $D = M_t^+ \ominus M_t$.
Let $c,c_1,\ldots,c_k$ be decomposition of $D$ on $\B$. 
And since $\B$ is a diff-set class, such decomposition exists.
Assume, without loss of generality, that $e\in c$.

By Lemma~Y, we know that 
\begin{equation}
	\label{eq:lemma-key-1}
	D_+ = c_+\cup c_1^+ \cup \ldots \cup c_k^+\quad \text{and}\quad D_- = c_-\cup c_1^-\cup \ldots c_k^-.
\end{equation}

We also denote $K=\rank(\B)$.


\textbf{Case (1).} 
Suppose that $c \in\Bopt$. Then $w(c)>0$.
Since $e\in E_t^3$, we have $\rad_t(e) \le \frac{1}{3}\Delta_e \le \frac{1}{3K}w(c)$.
In addition, $\forall g\in c_t, g\not=e$, $\rad_t(g) \le \rad_t(e)\le \frac{1}{3K}w(c)$.
Hence, $\rad_t(c) = \sum_{g\in c_t} \rad_t(g) \le \frac{|c_t|}{3K}w(c) \le \frac{1}{3}w(c)$.

Hence, $\bar w_t(c) \ge w(c)-\rad_t(c) \ge \frac{2}{3}w(c) > 0$.
This means that $\bar w_t(M_t \oplus c) = \bar w_t(M_t)+\bar w_t(c) > \bar w_t(M_t)$.
Therefore, $M_t \not= \max_{M\in \M} \bar w_t(M)$.
This contradicts to the definition of $M_t$.

\textbf{Case (2).} 
Suppose that $c_t\not\in \Bopt$. Then, one of the following mutually exclusive cases must hold.

\textbf{Case (2.1).} 
$(e \in M_* \wedge e\in c_+)$ or $(e \not \in M_* \wedge e\in c_-)$. 
 
Let the decomposition of $M_* \ominus (M_t \oplus D \ominus c)$ on $\B$ be $b,b_1,\ldots,b_l$, which exists due to $\B$ is a diff-set class. 
Assume wlog that $e\in b$. 
We write $b=(b_+,b_-)$.
It is easy to see that $b \in \Bopt$. 

Define $\tilde D = (M_t \oplus D \ominus c)\ominus M_t$ and $D' = (M_t \oplus \tilde D \oplus b) \ominus M_t$.
By Lemma~\ref{lemma:diff-set-algebra}, we know that $\tilde D = D\ominus c$ and $D'=\tilde D\oplus b$.
We also write $\tilde D=(\tilde D_+, \tilde D_-)$ and $D'= (D'_+, D'_-)$.
By definition, we have 
\begin{align*}
	\tilde D_+ &= (D_+ \cup c_-)\del (D_- \cup c_+)\\
	           &= (D_+\cup c_-\del D_-) \cap (D_+\cup c_-\del c_+)\\
	           &= D_+ \cap (D_+\del c_-) \\
	           &= D_+\del c_+.
\end{align*}
By the same method, we are able to show that $\tilde D_-=D_-\del c_-$.
Therefore we have 
\begin{equation}
\label{eq:2.1.0.1}
\tilde D_+\subseteq D_+\quad\text{and}\quad\tilde D_-\subseteq D_-.
\end{equation}

First, we show that $\rad_t(c) \le \frac13 w(b)$.
Since $e\in E_t^3$, $e\in b$ and $b\in \Bopt$, we have $\rad_t(e) \le \frac{1}{3}\Delta_e \le \frac{1}{3K}w(b)$.
In addition, $\forall g\in c, g\not=e$, $\rad_t(g) \le \rad_t(e)\le \frac{1}{3K}w(b)$.
Hence, 
\begin{align}
\rad_t(c) &= \sum_{g\in c} \rad_t(g) \nonumber \\
		  & \le \frac{|c|}{3K}w(b) \nonumber \\
		  &\le \frac{1}{3}w(b).\label{eq:2.1.0.2}
\end{align}

%Consider the diff-set $D'=D\ominus c \oplus b$.
Now, we show that $\rad_t(\tilde D_+ \cap b_-)+\rad_t(\tilde D_- \cap b_+)+ \le \frac13 w(b)$.
Since Eq.~\eqref{eq:2.1.0.1}, we have $\forall g\in (\tilde D_+\cap b_-)\cup(\tilde D_-\cap b_+), g\not=e$, 
$\rad_t(g) \le \rad_t(e)\le \frac{1}{3K}w(b)$.
Note that $|\tilde D_+\cap b_-|+|\tilde D_-\cap b_+|\le |b_+|+|b_-| \le K$. 
Hence, 
\begin{align}
\rad_t(\tilde D_+ \cap b_-)+\rad_t(\tilde D_- \cap b_+) &= \sum_{g\in (\tilde D_+\cap b_-)\cup(\tilde D_-\cap b_+)} \rad_t(g) \nonumber\\
	& \le \frac{K}{3K}w(b) \nonumber \\
	& \le \frac{1}{3}w(b). \label{eq:2.1.0.3}
\end{align}

Then, we have
\begin{align}
\rad_t(D')-\rad_t(D) &= \rad_t(\tilde D \oplus b)-\rad_t(D) \\
					 &= \rad_t(\tilde D)+\rad_t(b)-2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+)-\rad_t(D)
					 \label{eq:2.1.1}\\
					 &= \rad_t(D\ominus c)+\rad_t(b)-2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+)-\rad_t(D)\\
					 &= \rad_t(D)+\rad_t(c)+\rad_t(b)-2\rad_t(D_+\cap c_+)-2\rad_t(D_-\cap c_-) \nonumber \\
					 &\quad -2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+)-\rad_t(D)
					 \label{eq:2.1.2}\\
					 &= \rad_t(D)+\rad_t(c)+\rad_t(b)-2\rad_t(c_+)-2\rad_t(c_-) \nonumber \\
					 &\quad -2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+)-\rad_t(D)
					 \label{eq:2.1.3}\\
					 &= \rad_t(b)-\rad_t(c)-2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+),
\end{align}
where Eq.~\eqref{eq:2.1.1} and Eq.~\eqref{eq:2.1.2} follow from Lemma~\ref{lemma:diff-algebra-rad}, and 
Eq.~\eqref{eq:2.1.3} follows from Eq.~\eqref{eq:lemma-key-1}.

By the definition of $D$, we have that $w^+_t(D) \ge w^+_t(D')$. 
This means that 
\begin{align}
	\bar w_t(D)+\rad_t(D) &\ge \bar w_t(D')+\rad_t(D')\\
						  &= \bar w_t(D)-\bar w_t(c)+\bar w_t(b)+\rad_t(D').
\end{align}
By regrouping the above inequality, we have
\begin{align}
   \bar w_t(c) &\ge \bar w_t(b) + \rad_t(D')-\rad_t(D) \\
   			   &= \bar w_t(b)+\rad_t(b)-\rad_t(c)-2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+)\\
   			   &\ge w(b)-\rad_t(c)-2\rad_t(\tilde D_+\cap b_-)-2\rad_t(\tilde D_-\cap b_+) \\
   			   &> w(b)-\frac13 w(b)-\frac23 w(b) \label{eq:2.1.4}\\
   			   &= 0,
\end{align}
where Eq.~\eqref{eq:2.1.4} follows from Eq.~\eqref{eq:2.1.0.2} and Eq.~\eqref{eq:2.1.0.3}.

This contradicts to the definition of $M_t$.

\textbf{Case (2.2).}
$(e \in M_* \wedge e\in c_-)$ or $(e \not \in M_* \wedge e\in c_+)$.

Let the decomposition of $M_* \ominus (M_t\oplus D)$ on $\B$ be $b,b_1,\ldots, b_l$.
Assume wlog that $e\in b$.
We write that $b=(b_+,b_-)$.
Note that $b\in \Bopt$ and hence $w(b)>0$.

Define $D'=(M_t\oplus D\oplus b)\ominus M_t$. By Lemma~\ref{lemma:diff-set-algebra}, we know that $D' = D\oplus b$.

First, we show that $|D\del D'| \le |b|$.
Let $C=D\del D'$ and write $C=(C_+,C_-)$.
We can bound $|C_+|$ as follows.
\begin{align*}
	C_+ &= D_+\del D'_+\\
		&= D_+\del \left((D_+\cup b_+) \del (D_-\cup b_-)\right)\\
		&= (D_+ \cap (D_-\cup b_-)) \cup (D_+ \del (D_+\cup b_+))\\
		&= D_+\cap b_-.
\end{align*}
Hence, we have $|C_+| \le |b_-|$.
Then, we move to bounding $|C_-|$
\begin{align*}
	C_- &= D_-\del D'_-\\
		&= D_-\del \left((D_-\cup b_-) \del (D_+\cup b_+)\right)\\
		&= (D_- \cap (D_+\cup b_+)) \cup (D_- \del (D_-\cup b_-))\\
		&= D_-\cap b_+.
\end{align*}
Thus $|C_-|\le |b_+|$ and we proved that $|D\del D'|\le |b|$.


Next, we show that $\rad_t(D \del D') \le \frac13 w(b)$.
Since $e\in E_t^3$, $e\in b$ and $b\in \Bopt$, we have $\rad_t(e) \le \frac{1}{3}\Delta_e \le \frac{1}{3K}w(b)$.
In addition, $\forall g\in (D \del D'), g\not=e$, $\rad_t(g) \le \rad_t(e)\le \frac{1}{3K}w(b)$.
Note that $|D\del D'| \le |b| \le K$.
Hence, $\rad_t(D\del D') = \sum_{g\in (D\del D')} \rad_t(g) \le \frac{K}{3K}w(b) \le \frac{1}{3}w(b)$.

We also note that
\begin{align}
w(D'\del D)-w(D\del D') &= w(D' \del D)+w(D'\cap D)-w(D\cap D')-w(D\del D')\\
						&= w(D')-w(D) \\
						&= w(b),
\end{align}
where we have repeatedly applied Lemma~\ref{lemma:weight-diff-simple}.

Then, we show that $w^+_t(D') > w^+_t(D)$.
\begin{align}
	w_t^+(D')-w_t^+(D) &= \bar w_t(D')-\bar w_t(D)+\rad_t(D')-\rad_t(D)\\
					   &= \bar w_t(D'\del D)-\bar w_t(D\del D')+\rad_t(D'\del D)-\rad_t(D\del D') \label{eq:2.2.x.1} \\
					   &\ge w(D'\del D)-w(D\del D')-2\rad_t(D\del D') \label{eq:2.2.x.2} \\
					   &= w(b)-2\rad_t(D\del D')\\
					   &> w(b)-\frac23 w(b) \\
					   &= \frac13 w(b) > 0,
\end{align}
where Eq.~\eqref{eq:2.2.x.1} follows from Lemma~\ref{lemma:rad-diff-simple} and Eq.~\eqref{eq:2.2.x.2} follows from
the fact that $\bar w_t(D'\del D)+\rad_t(D'\del D)\ge w(D'\del D)$ and that $\bar w_t(D\del D')+\rad_t(D\del D')\ge w(D\del D')$, under the random event $\xi$.

This contradicts to the fact that $D$ is chosen on round $t$.
\end{proof}


\section{Proof of Lower Bounds}

\begin{lemma}
$$
\Delta_e = \min_{b: e \in b, b \in \Bopt} w(b).
$$
\end{lemma}

\begin{proof}
\end{proof}


\begin{proof}
Fix $\delta >0$, $\vec w =\{w(1),\ldots,w(n)\}$ and a $\delta$-correct policy $\mathbb A$.
For each $e\in [n]$, assume that the reward distribution is given by $\Rew_e=\mathcal N(w(e),1)$.
For any $e\in [n]$, let $T_e$ denote the  number of trials of arm $e$ used by algorithm $\mathbb A$.
In the rest of the proof, we will show that for any $e\in [n]$, the number of trials of arm $e$ is lower-bounded by
\begin{equation}
\E[T_e] \ge \frac{1}{16\Delta_e^2}\log(1/4\delta).
\label{eq:lower-each}
\end{equation}
Notice that the theorem follows immediately by summing up Eq.~\eqref{eq:lower-each} for all $e\in[n]$.


Fix an arm $e\in [n]$. We now focus on proving Eq.~\eqref{eq:lower-each}.
Consider two hypothesis $H_0$ and $H_1$. 
Under hypothesis $H_0$, all reward distributions are same with our assumption before
$$
H_0: \Rew_l = \mathcal N(w(l),1) \quad \text{for all } l \in [n].
$$
Under hypothesis $H_1$, we change the means of reward distributions such that 
$$
H_1: 
	\Rew_e = \begin{cases}
	\mathcal N(w(e)-2\Delta_e,1) & \text{if } e\in M_*\\
	\mathcal N(w(e)+2\Delta_e,1) & \text{if } e\not\in M_*
\end{cases} 
\quad\text{and } \Rew_l=\mathcal N(w(l), 1) \quad\text{for all } l\not = e.
$$

Define $M_e$ be the ``next-to-optimal'' set as follows 
$$
M_e = \begin{cases}
		 \argmax_{M\in \M: e \in M} w(M) & \text{if } e\not \in M_*, \\
	     \argmax_{M\in \M: e \not\in M} w(M) & \text{if } e\in M_*.
	  \end{cases}
$$
By definition of $\Delta_e$, we know that $w(M_*)-w(M_e)=\Delta_e$.

Let $\vec w_0$ and $\vec w_1$ be expected reward vectors under $H_0$ and $H_1$ respectively.
Notice that $w_0(M_*)-w_0(M_e)=\Delta_e > 0$.
On the other hand, 
$w_1(M_*)-w_1(M_e) = -\Delta < 0$.
This means that under $H_1$, $M_*$ is not the optimal set.
For $l\in \{0,1\}$, we use $\E_l$ and $\Pr_l$ to denote the expectation and probability, respectively, under the hypothesis $H_l$.

Define $\theta=4\delta$. Define
\begin{equation}
t_e^* = \frac{1}{16\Delta^2_e}\log\left(\frac{1}{\theta}\right).
\label{eq:define-tstar}
\end{equation}

Recall that $T_e$ denotes the total number of samples of arm $e$.
Define the event
$\mathcal A = \{T_e \le 4t_e^* \}$.

First, we show that $\Pr_0[\mathcal A] \ge 3/4$. 
This can be proved by Markov inequality as follows.
\begin{align*}
\Pr_0[T_e > 4t_e^*] &\le \frac{\E_0[T_e]}{4t_e^*} \\
					  &= \frac{t_e^*}{4t_e^*} = \frac14.
\end{align*}

Let $X_1,\ldots,X_{T_e}$ denote the sequence of reward outcomes of arm $e$.
We define $K_t(e)$ as the sum of outcomes of arm $e$ up to round $t$, i.e. $K_t(e) = \sum_{i\in [t]} X_i. $
Next, we define the event 
$$
\mathcal C=\left\{\max_{1\le t \le 4t_e^*} \left|K_t(e)-t\cdot w(e)\right|  < \sqrt{t_e^*\log(1/\theta)} \right\}.
$$
We now show that $\Pr_0[\mathcal C] \ge 3/4$.
First, notice that $K_t(e)-p_e t$ is a martingale under $H_0$.
Then, by Kolmogorov's inequality, we have
\begin{align*}
\Pr_0\left[\max_{1\le t \le 4t_e^*} \left|K_t(e)-t\cdot w(e)\right| \ge \sqrt{t_e^*\log(1/\theta)} \right]
&\le \frac{\E_0[ (K_{4t_e^*}(e)-4w(e)t_e^*)^2]}{t_e^*\log(1/\theta)}\\
&= \frac{4t_e^*}{t_e^*\log(1/\theta)}\\
&< \frac14,
\end{align*}
where the second inequality follows from the fact that $\E_0[(K_{4t_e^*}(e)-4w(e)t_e^*)^2] = 4t_e^*$; the last inequality follows 
since $\theta < e^{-16}$.

Then, we define the event $\mathcal B$ as the event that the algorithm eventually returns $M_*$, i.e.
$$
\mathcal B=\{\out=M_*\}.
$$
Since the probability of error of the algorithm is smaller than $\delta < 1/4$, we have $\Pr_0[\mathcal B] \ge 3/4$.
Define $\mathcal S$ be $\mathcal S=\mathcal A\cap \mathcal B \cap \mathcal C$. 
Then, by union bound, we have $\Pr_0[\mathcal S]\ge 1/4$.

Now, we show that if $\E_0[T_e] \le t_e^*$, then $\Pr_1[\mathcal B] \ge \delta$.
Let $W$ be the history of the sampling process until the algorithm stops (including the sequence of arms chosen at each time and the sequence of observed outcomes).
Define the likelihood function $L_l$ as 
$$
L_l(w) = p_l(W=w),
$$
where $p_l$ is the probability density function under hypothesis $H_l$.
Let $K$ be the shorthand of $K_e(T_e)$.

Assume that the event $\mathcal S$ occurred.
We will bound the likelihood ratio $L_1(W)/L_0(W)$ under this assumption. 
To do this, we divide our analysis into two different cases.

\textbf{Case (1): $e\not \in M_*$.}
In this case, the reward distribution of arm $e$ under $H_1$ is a Gaussian distribution with mean $p_e+2\Delta_e$ and variance 1. 
Recall that the probability density function of a Gaussian distribution with mean $\mu$ and variance $\sigma^2$ is given by
$\mathcal N(x | \mu,\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$.
Hence, we have
\begin{align}
  \frac{L_1(W)}{L_0(W)} &= \prod_{i=1}^{T_e} \exp\left(\frac{-(X_i-w(e)-2\Delta_e)^2+(X_i-w(e))^2}{2}\right) \nonumber \\
  						&= \prod_{i=1}^{T_e} \exp\big(\Delta_e(2X_i-2w(e))-2\Delta_e^2\big) \nonumber \\
  						&= \exp\big(\Delta_e(2K-2w(e)T_e)-2\Delta_e^2T_e\big) \nonumber \\
  						&= \exp\big(\Delta_e(2K-2w(e)T_e)\big)\exp(-2\Delta_e^2T_e) \label{eq:lower-bound-case-1}.
\end{align}

Next, we bound each individual term on the right-hand side of Eq.~\eqref{eq:lower-bound-case-1}.
We begin with bounding the second term of Eq.~\eqref{eq:lower-bound-case-1}
\begin{align}
	\exp(-2\Delta_e^2T_e) &\ge \exp(-8\Delta_e^2t_e^*) \label{eq:lower-bound-case-1-1.1} \\
						  &=\exp\left(-\frac{8}{16}\log(1/\theta)\right) \label{eq:lower-bound-case-1-1.2}\\
						  &= \theta^{1/2}\label{eq:lower-bound-case-1-1.3},
\end{align}
where Eq.~\eqref{eq:lower-bound-case-1-1.1} follows from the assumption that event $\mathcal S$ occurred, which implies that event $\mathcal A$ occurred and therefore $T_e \le 4t_e^*$; Eq.~\eqref{eq:lower-bound-case-1-1.2} follows from the definition of $t_e^*$.

Then, we bound the first term on the right-hand side of Eq.~\eqref{eq:lower-bound-case-1} as follows
\begin{align}
	\exp\big(\Delta_e(2K-2w(e)T_e)\big) & \ge \exp\left(-2\Delta_e\sqrt{t_e^*\log(1/\theta)}\right) \label{eq:lower-bound-case-1-2.1}\\
								       & = \exp\left(-\frac{2}{\sqrt{4}}\log(1/\theta)\right) \label{eq:lower-bound-case-1-2.2}\\
								       &=\theta^{1/2},  \label{eq:lower-bound-case-1-2.3}
\end{align}
where Eq.~\eqref{eq:lower-bound-case-1-2.1} follows from the assumption that event $\mathcal S$ occurred, which implies that event $\mathcal C$ and therefore $|2K-2w(e)T_e| \le \sqrt{t_e^*\log(1/\theta)}$; 
Eq.~\eqref{eq:lower-bound-case-1-2.2} follows from the definition of $t_e^*$.

Combining Eq.~\eqref{eq:lower-bound-case-1-1.3} and Eq.~\eqref{eq:lower-bound-case-1-2.3}, we can bound $L_1(W)/L_0(W)$ for this case as follows
\begin{equation}
\label{eq:ll-case1-final} 
\frac{L_1(W)}{L_0(W)} \ge \theta. 
\end{equation}


\emph{(End of Case (1).)}

\textbf{Case (2): $e\in M_*$.}
In this case, we know that the mean reward of arm $e$ under $H_1$ is $p_e-2\Delta$.
Therefore, the likelihood ratio $L_1(W)/L_0(W)$ is given by
\begin{align}
  \frac{L_1(W)}{L_0(W)} &= \prod_{i=1}^{T_e} \exp\left(\frac{-(X_i-w(e)+2\Delta_e)^2+(X_i-w(e))^2}{2}\right) \nonumber \\
  						&= \prod_{i=1}^{T_e} \exp\big(\Delta_e(2w(e)-2X_i)-2\Delta_e^2\big) \nonumber \\
  						&= \exp\big(\Delta_e(2w(e)T_e-2K)\big)\exp(-2\Delta_e^2T_e) \label{eq:lower-bound-case-2}.
\end{align}

Notice that the right-hand side of Eq.~\eqref{eq:lower-bound-case-2} differs from Eq.~\eqref{eq:lower-bound-case-1} only in its first term.
Now, we bound the first term as follows
\begin{align}
	\exp\big(\Delta_e(2K-2w(e)T_e)\big) & \ge \exp\left(-2\Delta_e\sqrt{t_e^*\log(1/\theta)}\right) \label{eq:lower-bound-case-2-2.1}\\
								       & = \exp\left(-\frac{2}{4}\log(1/\theta)\right) \label{eq:lower-bound-case-2-2.2}\\
								       &=\theta^{1/2},  \label{eq:lower-bound-case-2-2.3}
\end{align}
where the inequalities hold due to reasons similar to Case (1): Eq.~\eqref{eq:lower-bound-case-2-2.1} follows from the assumption that event $\mathcal S$ occurred, which implies that event $\mathcal C$ and therefore $|2K-2w(e)T_e| \le \sqrt{t_e^*\log(1/\theta)}$; 
Eq.~\eqref{eq:lower-bound-case-2-2.2} follows from the definition of $t_e^*$.

Combining Eq.~\eqref{eq:lower-bound-case-1-1.3} and Eq.~\eqref{eq:lower-bound-case-1-2.3}, we  can obtain the same bound of $L_1(W)/L_0(W)$ as in Eq.~\eqref{eq:ll-case1-final}, i.e. $L_1(W)/L_0(W) \ge \theta$.

\emph{(End of Case (2).)}

At this point, we have proved that, if the event $\mathcal S$ occurred, then the bound of likelihood ratio Eq.~\eqref{eq:ll-case1-final} holds, i.e. $\frac{L_1(W)}{L_0(W)} \ge \theta$.
Hence, we have
\begin{align}
\frac{L_1(W)}{L_0(W)} &\ge \theta \nonumber \\
					  &= 4\delta.	
\end{align}


Define $1_S$ as the indicator variable of event $\mathcal S$, i.e. $1_S = 1$ if and only if $\mathcal S$ occurs and otherwise $1_S = 0$.
Then, we have
\begin{align*}
\frac{L_1(W)}{L_0(W)} 1_S \ge 4\delta 1_S
\end{align*}
holds regardless the occurrence of event $\mathcal S$.
Therefore, we can obtain
\begin{align*}
\Pr_1[\mathcal B] &\ge \Pr_1[\mathcal S] = \E_1[1_S] \\
				  &= \E_0\left[\frac{L_1(W)}{L_0(W)} 1_S\right] \\
				  &\ge 4\delta \E_0[1_S] \\
				  &= 4\delta \Pr_0[\mathcal S] > \delta.
\end{align*}
Now we have proved that, if $\E_0[T_e] \le t_e^*$, then $\Pr_1[\mathcal B]>\delta$.
This means that, if $\E_0[T_e] \le t_e^*$, algorithm $\mathbb A$ will choose $M_*$ as the output with probability at least $\delta$, under hypothesis $H_1$.
However, under $H_1$, we have shown that $M_*$ is not the optimal set since $w_1(M_e) > w_1(M_*)$.
Therefore, algorithm $\mathbb A$ has a probability of error larger than $\delta$ under $H_1$. 
This contradicts to the assumption that algorithm $\mathbb A$ is a $\delta$-correct algorithm.
Hence, we must have $\E_0[T_e] > t_e^* = \frac{1}{16\Delta_e^2}\log(1/4\delta)$.

\end{proof}

\begin{proof}
Fix $\delta >0$, $\vec w\in \RR^{n}$, diff-set $b=(b_+,b_-)$ and a $\delta$-correct algorithm $\mathbb A$.
Assume that $\Rew_e(e)=\mathcal N(w(e),1)$ for all $e\in[n]$.

%the reward distribution of an arm $i\in [n]$ is a Gaussian distribution with mean $p_i$ and variance 1.


We define three hypotheses $H_0$, $H_1$ and $H_2$. 
%Under each of these hypotheses, the reward distribution of each arm is Gaussian with different means. 
Under hypothesis $H_0$, the reward distribution 
$$
H_0: \Rew_l = \mathcal N(w(l),1) \quad \text{for all } l \in [n].
$$
Under hypothesis $H_1$, the mean reward of each arm is given by 
$$
H_1: \Rew_e = \begin{cases}
	\mathcal N\left(w(e)+2\frac{w(b)}{|b_-|},1\right) & \text{if } e\in b_-,\\
	\mathcal N(w(e), 1) & \text{if } e\not\in b_-.\\
    \end{cases}
$$
And under hypothesis $H_2$, the mean reward of each arm is given by 
$$
H_2: q_e = \begin{cases}
	\mathcal N\left(w(e)-2\frac{w(b)}{|b_-|},1\right)  & \text{if } e\in b_+,\\
	\mathcal N(w(e), 1)  & \text{if } e\not\in b_+.\\
    \end{cases}
$$

Since $b\in \Bopt$, it is clear that $\neg b \diffvalid M_*$. Hence we define $M = M_* \ominus b$.
Let $w_0, w_1$ and $w_2$ be the expected reward vectors under $H_0,H_1$ and $H_2$ respectively.
It is easy to check that 
$w_1(M_*)-w_1(M) = -w(b) < 0$ and
$w_2(M_*)-w_2(M) = -w(b) < 0$.
This means that under $H_1$ or $H_2$, $M_*$ is not the optimal set.
Further, for $l\in \{0,1,2\}$, we use $\E_l$ and $\Pr_l$ to denote the expectation and probability, respectively, under the hypothesis $H_l$.
In addition, let $W$ be the history of the sampling process until algorithm $\mathbb A$ stops.
Define the likelihood function $L_l$ as 
$$
L_l(w) = p_l(W=w),
$$
where $p_l$ is the probability density function under $H_l$.


Define $\theta=4\delta$.
Let $T_{b_-}$ and $T_{b_+}$ denote the number of trials of arms belonging to $b_-$ and $b_+$, respectively. 
In the rest of the proof, we will bound $\E_0[T_{b_-}]$ and $\E_0[T_{b_+}]$ individually.



\textbf{Part (1): Lower bound of $\E_0[T_{b_-}]$.}
In this part, we will show that $\E_0[T_{b_-}]\ge t_{b_-}^*$, where we define $t_{b_-}^* = \frac{|b_-|^2}{16 w(b)^2}\log(1/\theta)$.

Consider the complete sequence of sampling process by algorithm $\mathbb A$.
Formally, let $W=\{(\tilde I_1,\tilde X_1),\ldots, (\tilde I_T, \tilde X_T)\}$ be the sequence of all trials by algorithm $\mathbb A$, where $\tilde I_i$ denotes the arm played in $i$-th trial and $\tilde X_i$ be the reward outcome of $i$-th trial.
Then, consider the subsequence $W_1$ of $W$ which consists all the trials of arms in $b_-$.
Specifically, we write $W=\{(I_1,X_1),\ldots,(I_{T_{b_-}}, X_{T_{b_-}})\}$ such that $W_1$ is a subsequence of $W$ and $I_i \in b_-$ for all $i$.

Next, we define several random events in a way similar to the proof of Theorem~\ref{theorem:lower-bound}.
Define event
$\mathcal A_1 = \{T_{b_-} \le 4t_{b_-}^* \}$.
Define event 
$$
\mathcal C_1 = \left\{\max_{1\le t \le 4t_{b_-}^*} \left|\sum_{i=1}^t X_i - \sum_{i=1}^t w(I_i)\right|  < \sqrt{t_{b_-}^*\log(1/\theta)} \right\}.
$$
Define event 
\begin{equation}
\label{eq:lower-sum-b-define}
\mathcal B = \{\out=M_*\}.
\end{equation}
Define event
$\mathcal S_1 = \mathcal A_1 \cap \mathcal B \cap \mathcal C_1$.
Then, we bound the probability of events $\mathcal A_1$, $\mathcal B$, $\mathcal C_1$ and $\mathcal S_1$ under $H_0$ using methods similar to Theorem~\ref{theorem:lower-bound}.
First, we show that $\Pr_0[\mathcal A_1] \ge 3/4$. 
This can be proved by Markov inequality as follows.
\begin{align*}
\Pr_0[T_{b_-} > 4t_{b_-}^*] &\le \frac{\E_0[T_{b_-}]}{4t_{b_-}^*} \\
					  &= \frac{t_{b_-}^*}{4t_{b_-}^*} = \frac14.
\end{align*}
Next, we show that $\Pr_0[\mathcal C_1] \ge 3/4$.
Notice that the sequence $\Big\{\sum_{i=1}^t X_i - \sum_{i=1}^t p_{I_i}\Big\}_{t\in[4t_{b_-}^*]}$ is a martingale.
Hence, by Kolmogorov's inequality, we have
\begin{align*}
\Pr_0\left[\max_{1\le t \le 4t_{b_-}^*} \left|\sum_{i=1}^t X_i - \sum_{i=1}^t w(I_i) \right| \ge \sqrt{t_e^*\log(1/\theta)} \right]
&\le \frac{\E_0\left[ \left(\sum_{i=1}^{4t_{b_-}^*} X_i - \sum_{i=1}^{4t_{b_-}^*} w(I_i)\right)^2\right]}{t_e^*\log(1/\theta)}\\
&= \frac{4t_{b_-}^*}{t_{b_-}^*\log(1/\theta)}\\
&< \frac14,
\end{align*}
where the second inequality follows from the fact that all reward distributions have unit variance and hence
$\E_0\left[ \left(\sum_{i=1}^{4t_{b_-}^*} X_i - \sum_{i=1}^{4t_{b_-}^*} p_{I_i}\right)^2\right] = 4t_{b_-}^*$; the last inequality follows 
since $\theta < e^{-16}$.
Last, since algorithm $\mathbb A$ is a $\delta$-correct algorithm with $\delta < 1/4$. 
Therefore, it is easy to see that 
$\Pr_0[\mathcal B] \ge 3/4$.
And by union bound, we have
$$
\Pr_0[\mathcal S_1] \ge 1/4.
$$

Now, we show that if $\E_0[T_{b_-}] \le t_{b_-}^*$, then $\Pr_1[\mathcal B] \ge \delta$.
Assume that the event $\mathcal S_1$ occurred.
%Define $K(e) = \sum_{t=1}^{T_e} $ 
We bound the likelihood ratio $L_1(W)/L_0(W)$ under this assumption as follows
\begin{align}
  \frac{L_1(W)}{L_0(W)} 
  &= \prod_{i=1}^{T_{b_-}}
  \exp\left(\frac{-\left(X_i-w(I_i)-\frac{2w(b)}{|b_-|}\right)^2+(X_i-w(I_i)^2}{2}\right) \nonumber \\
  &= \prod_{i=1}^{T_{b_-}}
  \exp\left(\frac{w(b)}{|b_-|}(2X_i-2w(I_i))-\frac{2w(b)^2}{|b_-|^2}\right) \nonumber \\
  &= \exp\left(\frac{w(b)}{|b_-|}\left(\sum_{i=1}^{T_{b_-}}2X_i-2w(I_i)\right)-\frac{2w(b)^2}{|b_-|^2}T_{b_-}\right) \nonumber \\
  &= \exp\left(\frac{w(b)}{|b_-|}\left(\sum_{i=1}^{T_{b_-}}2X_i-2w(I_i)\right)\right)\exp\left(-\frac{2w(b)^2}{|b_-|^2}T_{b_-}\right) \label{eq:lower-sum-case-1}.
\end{align}
Then, we bound each term on the right-hand side of Eq.~\eqref{eq:lower-sum-case-1}.
First, we bound the second term of Eq.~\eqref{eq:lower-sum-case-1}.
\begin{align}
	\exp\left(-\frac{2w(b)^2}{|b_-|^2} T_{b_-}\right) 
	 &\ge \exp\left(-\frac{2w(b)^2}{|b_-|^2} 4t_b^*\right) \label{eq:lower-sum-case-1-1.1} \\
     &=\exp\left(-\frac{8}{16}\log(1/\theta)\right) \label{eq:lower-sum-case-1-1.2}\\
     &= \theta^{1/2}\label{eq:lower-sum-case-1-1.3},
\end{align}
where Eq.~\eqref{eq:lower-sum-case-1-1.1} follows from the assumption that events $\mathcal S_1$ and $\mathcal A_1$ occurred and therefore $T_{b_-} \le 4t_{b_-}^*$; 
Eq.~\eqref{eq:lower-sum-case-1-1.2} follows from the definition of $t_{b_-}^*$.
Next, we bound the first term of Eq.~\eqref{eq:lower-sum-case-1} as follows
\begin{align}
	\exp\left(\frac{w(b)}{|b_-|}\left(\sum_{i=1}^{T_{b_-}}2X_i-2w(I_i)\right)\right)
	&\ge \exp\left(-\frac{2w(b)}{|b_-|}\sqrt{t_b^*\log(1/\theta)}\right) \label{eq:lower-sum-case-1-2.1}\\
    & = \exp\left(-\frac{2}{4}\log(1/\theta)\right) \label{eq:lower-sum-case-1-2.2}\\
    &=\theta^{1/2},  \label{eq:lower-sum-case-1-2.3}
\end{align}
where Eq.~\eqref{eq:lower-sum-case-1-2.1} follows since event $\mathcal S_1$ and $\mathcal C_1$ occurred and therefore $|2K-2p_eT_e| \le \sqrt{t_e^*\log(1/\theta)}$; 
Eq.~\eqref{eq:lower-sum-case-1-2.2} follows from the definition of $t_{b_-}^*$.

Hence, if event $\mathcal S_1$ occurred, we can bound the likelihood ratio as follows
\begin{align}
\frac{L_1(W)}{L_0(W)} &\ge \theta = 4\delta.
\end{align}
Let $1_{S_1}$ denote the indicator variable of event $\mathcal S_1$.
Then, we have $
\frac{L_1(W)}{L_0(W)} 1_{S_1} \ge 4\delta 1_{S_1}$.
Therefore, we can bound $\Pr_1[\mathcal B]$ as follows
\begin{align}
\Pr_1[\mathcal B] &\ge \Pr_1[\mathcal S_1] = \E_1[1_{S_1}] \nonumber \\
				  &= \E_0\left[\frac{L_1(W)}{L_0(W)} 1_{S_1}\right] \nonumber \\
				  &\ge 4\delta \E_0[1_{S_1}] \nonumber \\
				  &= 4\delta \Pr_0[\mathcal S_1] > \delta \label{eq:lower-sum-case-1-final}.
\end{align}
This means that, if $\E_0[T_{b_-}] \le t_{b_-}^*$, then, under $H_1$, the probability of algorithm $\mathbb A$ returning $M_*$ as output is at least $\delta$. 
But $M_*$ is not the optimal set under $H_1$. Hence this contradicts to the assumption that $\mathbb A$ is a $\delta$-correct algorithm.
Hence we have proved that 
\begin{equation}
\label{eq:lower-sum-case-1-a}
\E_0[T_{b_-}] \ge t_{b_-}^* = \frac{|b_-|^2}{16 w(b)^2}\log(1/4\delta).
\end{equation}

\emph{(End of Part (1).)}

\textbf{Part (2): Lower bound of $\E_0[T_{b_+}]$.} In this part, we will show that $\E_0[T_{b_+}]\ge t_{b_+}^*$, where we define $t_{b_+}^* = \frac{|b_+|^2}{16 w(b)^2}\log(1/\theta)$.
The arguments used in this part are similar to that of Part (1). 
Hence, we will omit the redundant parts and highlight the differences.

Recall that we have defined that $W$ to be the history of all trials by algorithm $\mathbb A$.
We define $W$ be the subsequence of $\tilde S$ which contains the trials of arms belonging to $b_+$.
We write $S_2=\{(J_1,Y_1),\ldots,(J_{T_{b_+}}, Y_{T_{b_+}})\}$, where $J_i$ is $i$-th played arm in sequence $S_2$ and $Y_i$ is the associated reward outcome.

We define the random events $\mathcal A_2$ and $\mathcal C_2$ similar to Part (1).
Specifically, we define 
$$
\mathcal A_2 = \{T_{b_+} \le 4t_{b_+}^* \} \quad\text{and}\quad
\mathcal C_2 = \left\{\max_{1\le t \le 4t_{b_+}^*} \left|\sum_{i=1}^t Y_i - \sum_{i=1}^t w(J_i)\right|  < \sqrt{t_{b_+}^*\log(1/\theta)} \right\}.
$$
Using the similar arguments, we can show that 
$\Pr_0[\mathcal A_2] \ge 3/4$ and $\Pr_0[\mathcal C_2] \ge 3/4$.
Define event $\mathcal S_2 = \mathcal A_2 \cap \mathcal B \cap \mathcal C_2$, where $\mathcal B$ is defined in Eq.~\eqref{eq:lower-sum-b-define}.
By union bound, we see that 
$$
\Pr_0[\mathcal S_2] \ge 1/4.
$$

Then, we show that if $\E_0[T_{b_+}] \le t_{b_+}^*$, then $\Pr_2[\mathcal B] \ge \delta$.
We bound likelihood ratio $L_2(W)/L_0(W)$ under the assumption that $\mathcal S_2$ occurred as follows
\begin{align}
  \frac{L_2(W)}{L_0(W)} 
  &= \prod_{i=1}^{T_{b_+}}
  \exp\left(\frac{-\left(Y_i-w(J_i))+\frac{2w(b)}{|b_-|}\right)^2+(Y_i-w(J_i))^2}{2}\right) \nonumber \\
  &= \prod_{i=1}^{T_{b_+}}
  \exp\left(\frac{w(b)}{|b_+|}(2w(J_i)- 2Y_i)-\frac{2w(b)^2}{|b_+|^2}\right) \nonumber \\
  &= \exp\left(\frac{w(b)}{|b_+|}\left(\sum_{i=1}^{T_{b_+}}2w(J_i)-2Y_i\right)-\frac{2w(b)^2}{|b_+|^2}T_{b_+}\right) \nonumber \\
  &= \exp\left(\frac{w(b)}{|b_+|}\left(\sum_{i=1}^{T_{b_+}}2w(J_i)-2Y_i\right)\right)\exp\left(-\frac{2w(b)^2}{|b_+|^2}T_{b_+}\right) \nonumber \\
  &\ge \theta \label{eq:lower-sum-case-2}\\
  & = 4\delta \nonumber,
\end{align}
where Eq.~\eqref{eq:lower-sum-case-2} can be obtained using same method as in Part (1) as well as the assumption that $\mathcal S_2$ occurred.

Next, similar to the derivation in Eq.~\eqref{eq:lower-sum-case-1-final}, we see that
$$
\Pr_2[\mathcal B] \ge \Pr_2[\mathcal S_2] = \E_2[1_{S_2}] 
				  = \E_0\left[\frac{L_2(W)}{L_0(W)} 1_{S_2}\right]
				  \ge 4\delta \E_0[1_{S_2}] > \delta,
$$
where $1_{S_2}$ is the indicator variable of event $\mathcal S_2$.
Therefore, we see that if $\E_0[T_{b_+}] \le t_{b_+}^*$, then, under $H_2$, the probability of algorithm $\mathbb A$ returning $M_*$ as output is at least $\delta$, which is not the optimal set under $H_2$. 
This contradicts to the assumption that algorithm $\mathbb A$ is a $\delta$-correct algorithm. 
In sum, we have proved that 
\begin{equation}
\label{eq:lower-sum-case-2-a}
\E_0[T_{b_+}] \ge t_{b_+}^* = \frac{|b_+|^2}{16 w(b)^2}\log(1/4\delta).
\end{equation}

\emph{(End of Part (2))}

Finally, we combine the results from both parts, i.e. Eq.~\eqref{eq:lower-sum-case-1-a} and Eq.~\eqref{eq:lower-sum-case-2-a}.
We obtain
\begin{align*}
  \E_0[T_b] &= \E_0[T_{b_-}]+\E_0[T_{b_+}] \\
  		    &\ge \frac{|b_+|^2+|b_-|^2}{16 w(b)^2}\log(1/4\delta) \\
  		    &\ge \frac{|b|^2}{32 w(b)^2}\log(1/4\delta).
\end{align*}
\end{proof}

Now we prove a lower bound on the probability of error in the fixed budget setting.
In particular, we show that for any expected rewards $\{w(1),\ldots, w(n)\}$ and any fixed budget algorithm $\mathbb A$ for pure exploration combinatorial bandit problem with feasible sets $\M$. 
We show that one can slightly modify the vector $\{w(e)\}_{e\in [n]}$ to construct another vector $\{\tilde w(1),\ldots,\tilde w(n)\}$ such that the probability of error of the fixed algorithm on a \Problem problem with expected rewards given by $\tilde w$ is at least $\Omega(\exp(n/\mathbf H(w)))$.
%In addition, newthe modified vector $\{\tilde w(1)\}$
%On the other hand, it is clear that one cannot prove any non-trivial lower bounds on the probability of error of algorithm $\mathbb A$: an algorithm which always outputs $M_*$ has zero probability of error for this particular problem instance.
%We also notice that Theorem~\ref{theorem:lower-bound} does not imply 


\begin{theorem}
Given a vector $\{w(1),\ldots, w(n)\}$, a budget $T>0$ and a collection of feasible sets $\M \subseteq 2^{[n]}$.
Let $\mathbb A$ be an arbitrary algorithm for $\M$-\Problem problem which uses at most $T$ samples.
There exists a vector $\{\tilde w(1), \ldots, \tilde w(n)\}$ such that $\mathbf H(\tilde w) \le 2\mathbf H(w)$ and satisfies the following property.
Consider the bandit problem with reward distributions  defined by $\Rew_e = \mathcal N(\tilde w(e), 1)$ for all $e\in[n]$, where $\mathcal N(\mu,\sigma^2)$ denotes Gaussian distribution with mean $\mu$ and variance $\sigma^2$.
The probability of error of $\mathbb A$ on this bandit problem satisfies
$$
\Pr\left[\out\not=M_* \right] \ge \exp\left(-\frac{T}{\mathbf H(w)}\right),
$$
where $\out$ is the output of $\mathbb A$ and $M_*=\argmax_{M\in \M} w(M)$ is the optimal set. 
In addition, vector $\{\tilde w(1),\ldots, \tilde w(n)\}$ differs from vector $\{w(1),\ldots,w(n)\}$ on exactly one index.
\end{theorem}

\begin{proof}
Fix $\M \subseteq 2^{[n]}$, $w(e)$ for all $e\in[n]$ and a fixed budget algorithm $\mathbb A$ for $\M$-\Problem problem.
Let $\sigma(1),\ldots,\sigma(n)$ be a permutation of $1,2,\ldots, n$ such that
 $\Delta_{\sigma(1)} \le \Delta_{\sigma(2)} \ldots \le \Delta_{\sigma(n)}$.
Define $L' = \argmax_{i\in [n]} i/\Delta_{\sigma(i)}^2$ and $L=\sigma(L')$.

Then, we construct hypothesis $H_0$ as follows
$$
H_0: \Rew_e = \mathcal N(w(e), 1) \quad \text{for all } e \in [n].
$$

We define random event $\mathcal C$ as follows.

We show that $\Pr_0[\mathcal C] \ge 1/2$.

We define random variables $X,Y,Z$ as follows
$$
X=\argmin_{i\in [L]\del \out} T_i, \quad Y=\argmin_{i\in [L]\cap \out} T_i \quad\text{and }
Z=\argmin_{i\in [L]} T_i, 
$$
where, for convenience, if $[L]\del \out=\emptyset$, we set $X=0$; and if $[L]\cap \out = \emptyset$, we set $Y=0$. 
By definition, we see that $X\not=Y$, $Z\in \{X,Y\}$ and $Z\not = 0$.
Now, by summing up all possible values of $X$, $Y$ and $Z$, we have
\begin{align*}
1/2 < \Pr_0[\mathcal C]
    = \sum_{\substack{x\in \{0,\ldots,L\}\\y\in \{0,\ldots, L\}\\x\not=y, z\in \{x,y\}}}
      \Pr_0[\mathcal C \cap \{X=x, Y=y, Z=z\}].
\end{align*}
Since a maximal is larger than an average, we see that there exists $x,y,z$ such that $x\not=y, z\in\{x,y\}$ and
\begin{equation}
\label{eq:lower-budget-z-prob}
\Pr_0[\mathcal C \cap \{X=x,Y=y,Z=z\}] \ge \frac{1}{4L(L+1)}.
\end{equation}
We point out that $x, y$ and $z$ are deterministic and only depends $\mathbb A$, $w$ and $\mathcal M$.
Now, depending on the value of $x,y$ and $z$, we divide our analysis into two cases.

\textbf{Case (1): $(z=x \wedge x\in M_*)$ or $(z=y \wedge y\not\in M_*)$.} 
Eq.~\eqref{eq:lower-budget-z-prob} implies that 
\begin{align*}
\Pr_0[\{X=x,Y=y,Z=z\}] &\ge \Pr_0[\mathcal C \cap \{X=x,Y=y,Z=z\}] \\
					   &\ge \frac{1}{4L(L+1)} \ge G. 
\end{align*}


First, let us assume that $z=x$ and $x\in M_*$.
By definition, we have $X\not\in\out$.
Notice that $x$ belong $M_*$. 
Therefore the event that $X=x$ and the assumption that $x\in M_*$ imply that $\out\not=M_*$.
This means that, if $z=x$ and $x\in M_*$, 
then $\Pr_0[\out\not=M_*] \ge \Pr_0[X=x] \ge G$.

Next, we assume that $z=y$ and $y\not \in M_*$.
Notice that $Y\in\out$ and $y\not\in M_*$.
Hence, the event $Y=y$ and the assumption that $y\not\in M_*$ imply that $\out\not=M_*$.
Therefore, if $z=y$ and $y\not\in M_*$, then
$\Pr_0[\out\not=M_*] \ge \Pr_0[Y=y] \ge  G$.

Therefore, we proved that, in Case (1), the probability of error of algorithm $\mathbb A$ is larger than $G$ under $H_0$.

\textbf{Case (2): $(z=x \wedge x\not \in M_*)$ or $(z=y \wedge y\in M_*)$.}
By definition, $Z$ is the arm with smallest number of samples among arms in $[L]$ and algorithm $\mathbb A$ uses at most $T$ samples. 
Therefore, we have
\begin{equation}
\label{eq:lower-budget-z-sample}
T_Z \le \frac{T}{L}.
\end{equation}

Then, we consider two cases separately.

\textbf{Case (2.1): $(z=x \wedge x\not \in M_*)$.}
We construct hypothesis $H_1$ as follows
$$
H_1: \Rew_x = \mathcal N(w(x)+\Delta_L+\varepsilon, 1) \quad\text{and}\quad
\Rew_e(e) = \mathcal N( w(e), 1) \quad \text{for all } e\not=y.
$$
Notice that, by the choice of $L$, we have $\Delta_x \ge \Delta_L$.
Hence we see that $w_1(M_x) = w_0(M_x)+\Delta_L \ge w_0(M_x)+\Delta_x = w_0(M_*)  = w_1(M_*)$.
Therefore, under $H_1$, $M_*$ is not the optimal set.
Now we bound the likelihood ratio $L_1(W)/L_0(W)$ as follows
\begin{align}
\frac{L_1(W)}{L_0(W)}
&= \prod_{i=1}^{T_Z} \exp\left(\frac{-(X_i-w(x)-\Delta_L)^2+(X_i-w(x))^2}{2}\right) \nonumber \\
&= \prod_{i=1}^{T_Z} \exp\big(\Delta_L(X_i-w(x))-\Delta_L^2\big) \nonumber \\
&= \exp\big(\Delta_L(K-T_Zw(x))-\Delta_L^2T_Z\big) \nonumber \\
&= \exp\big(\Delta_L(K-T_Zw(x))\big)\exp(-\Delta_L^2T_Z) \label{eq:lower-budget-case-1}.
\end{align}

Then we analyze the right-hand side of Eq.~\eqref{eq:lower-budget-case-1} as follows
\begin{align}
  \exp(-\Delta_Z^2T_Z) &\ge \exp(-\Delta_Z^2T/L) \label{eq:lower-bound-case-1-1.1} \\
					   &=\exp\left(-\frac{8}{16}\log(1/\theta)\right) \label{eq:lower-bound-case-1-1.2}\\
					   &= \theta^{1/2}\label{eq:lower-bound-case-1-1.3},
\end{align}
where Eq.~\eqref{eq:lower-bound-case-1-1.1} follows from the assumption that event $\mathcal S$ occurred, which implies that event $\mathcal A$ occurred and therefore $T_e \le 4t_e^*$; Eq.~\eqref{eq:lower-bound-case-1-1.2} follows from the definition of $t_e^*$.

Then, we bound the first term on the right-hand side of Eq.~\eqref{eq:lower-bound-case-1} as follows
\begin{align}
	\exp\big(\Delta_e(2K-2p_eT_e)\big) & \ge \exp\left(-2\Delta_e\sqrt{t_e^*\log(1/\theta)}\right) \label{eq:lower-bound-case-1-2.1}\\
								       & = \exp\left(-\frac{2}{\sqrt{4}}\log(1/\theta)\right) \label{eq:lower-bound-case-1-2.2}\\
								       &=\theta^{1/2},  \label{eq:lower-bound-case-1-2.3}
\end{align}
where Eq.~\eqref{eq:lower-bound-case-1-2.1} follows from the assumption that event $\mathcal S$ occurred, which implies that event $\mathcal C$ and therefore $|2K-2p_eT_e| \le \sqrt{t_e^*\log(1/\theta)}$; 
Eq.~\eqref{eq:lower-bound-case-1-2.2} follows from the definition of $t_e^*$.

\textbf{Case (2.2): $y\not\in M_*$.}
By definition of $y$, we see that the event $Y=y$ implies that $y\not\in\out$ and therefore $\out\not=M_*$.
On the other hand, using Eq.~\eqref{eq:lower-budget-z-prob}, we have
$$\Pr_0[Y=y] \ge \Pr_0[\mathcal C \cap \{Y=y, Z=0\} ] = \frac{1}{2(L+1)^2}.$$
This gives that $\Pr_0[\out\not=M_*] \ge \frac{1}{2(L+1)^2} \ge A$.


\end{proof}


\bibliography{bandit}
\bibliographystyle{plain}
\end{spacing}
\end{document}